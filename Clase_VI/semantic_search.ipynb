{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7Lc9I6taO3k"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/semantic-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/docs/semantic-search.ipynb)\n",
    "\n",
    "# Semantic Search\n",
    "\n",
    "In this walkthrough we will see how to use Pinecone for semantic search. To begin we must install the required prerequisite libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2h_LYPGpYsY0"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q03L1BYEZQfe"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -q \\\n",
    "  numpy==1.26.4 \\\n",
    "  pinecone-client==3.1.0 \\\n",
    "  pinecone-datasets==0.7.0 \\\n",
    "  sentence-transformers==3.3.0 \\\n",
    "  pinecone-notebooks==0.1.1 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xmobBcgqKEL"
   },
   "source": [
    "---\n",
    "\n",
    "ðŸš¨ _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrSfFiIC5roI"
   },
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kujS_e8s55oJ"
   },
   "source": [
    "In this notebook we will skip the data preparation steps as they can be very time consuming and jump straight into it with the prebuilt dataset from *Pinecone Datasets*. If you'd rather see how it's all done, please refer to [this notebook](https://github.com/pinecone-io/examples/blob/master/learn/search/semantic-search/semantic-search.ipynb).\n",
    "\n",
    "Let's go ahead and download the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w49zdZtqZH53"
   },
   "source": [
    "Quora es una plataforma de preguntas y respuestas en lÃ­nea donde los usuarios pueden hacer preguntas y proporcionar respuestas.\n",
    "epresentaciones vectoriales (embeddings) de preguntas utilizando el modelo [MiniLM-L6](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lOgjRG52Zqqz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"D:\\Users\\juanp_schamun\\AppData\\Local\\Temp\\ipykernel_14544\\758477735.py\", line 1, in <module>\n",
      "    from pinecone_datasets import load_dataset\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\pinecone_datasets\\__init__.py\", line 8, in <module>\n",
      "    from .dataset import Dataset, DatasetInitializationError\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\pinecone_datasets\\dataset.py\", line 10, in <module>\n",
      "    import pyarrow.parquet as pq\n",
      "  File \"d:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone_datasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquora_all-MiniLM-L6-bm25\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# we drop metadata as will use blob column\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\pinecone_datasets\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. include:: ../README.md\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.6.3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetInitializationError\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_datasets, load_dataset\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcatalog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetMetadata, DenseModelMetadata\n",
      "File \u001b[1;32md:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\pinecone_datasets\\dataset.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ValidationError\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Generator, Iterator, List, Dict, Optional, Tuple, NamedTuple\n",
      "File \u001b[1;32md:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\pyarrow\\__init__.py:65\u001b[0m\n\u001b[0;32m     63\u001b[0m _gc_enabled \u001b[38;5;241m=\u001b[39m _gc\u001b[38;5;241m.\u001b[39misenabled()\n\u001b[0;32m     64\u001b[0m _gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_lib\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _gc_enabled:\n\u001b[0;32m     67\u001b[0m     _gc\u001b[38;5;241m.\u001b[39menable()\n",
      "File \u001b[1;32md:\\Users\\juanp_schamun\\AppData\\Local\\anaconda3\\envs\\llm-curso\\lib\\site-packages\\pyarrow\\lib.pyx:37\u001b[0m, in \u001b[0;36minit pyarrow.lib\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# from pinecone_datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset('quora_all-MiniLM-L6-bm25')\n",
    "# # we drop metadata as will use blob column\n",
    "# dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
    "# dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
    "# # we will use 80K rows of the dataset between rows 240K -> 320K\n",
    "# dataset.documents.drop(dataset.documents.index[320_000:], inplace=True)\n",
    "# dataset.documents.drop(dataset.documents.index[:240_000], inplace=True)\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conuh2Uo-mwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebd7XSamfMsC"
   },
   "source": [
    "## Creating an Index\n",
    "\n",
    "Now the data is ready, we can set up our index to store it.\n",
    "\n",
    "We begin by initializing our connection to Pinecone. To do this we need a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EaGXuLz_zU7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# initialize connection to pinecone (orget API key at app.pinecone.io)\n",
    "if not os.environ.get(\"PINECONE_API_KEY\"):\n",
    "    from pinecone_notebooks.colab import Authenticate\n",
    "    Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mc66NEBAcQHY"
   },
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xknsDRh15-by"
   },
   "source": [
    "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0BUsx0E590t"
   },
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdaTip6CfllN"
   },
   "source": [
    "Now we create a new index called `semantic-search-fast`. It's important that we align the index `dimension` and `metric` parameters with those required by the `MiniLM-L6` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMmTOTzH6LNA",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "index_name = 'semantic-search-fast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kT8pfoO46Iwg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=384,  # dimensionality of minilm\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUd1VGg6i108"
   },
   "source": [
    "Upsert the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhR6WOi1huXZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [02:46<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for batch in tqdm(dataset.iter_documents(batch_size=500), total=160):\n",
    "    index.upsert(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrK_IN079Vuu"
   },
   "source": [
    "## Making Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr4unPAq9alb"
   },
   "source": [
    "Now that our index is populated we can begin making queries. We are performing a semantic search for *similar questions*, so we should embed and search with another question. Let's begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fqo_hMRZiubM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP2-unZ--XJ9"
   },
   "source": [
    "Now let's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWcO7jAK-N_1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '69331',\n",
       "              'metadata': {'text': \" What's the world's largest city?\"},\n",
       "              'score': 0.78565526,\n",
       "              'values': []},\n",
       "             {'id': '69332',\n",
       "              'metadata': {'text': ' What is the biggest city?'},\n",
       "              'score': 0.727139473,\n",
       "              'values': []},\n",
       "             {'id': '84749',\n",
       "              'metadata': {'text': \" What are the world's most advanced \"\n",
       "                                   'cities?'},\n",
       "              'score': 0.709211528,\n",
       "              'values': []},\n",
       "             {'id': '109231',\n",
       "              'metadata': {'text': ' Where is the most beautiful city in the '\n",
       "                                   'world?'},\n",
       "              'score': 0.696055,\n",
       "              'values': []},\n",
       "             {'id': '109230',\n",
       "              'metadata': {'text': ' What is the greatest, most beautiful city '\n",
       "                                   'in the world?'},\n",
       "              'score': 0.657444596,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"which city has the highest population in the world?\"\n",
    "\n",
    "# create the query vector\n",
    "xq = model.encode(query).tolist()\n",
    "\n",
    "# now query\n",
    "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
    "xc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XwOWcgo_QtI"
   },
   "source": [
    "In the returned response `xc` we can see the most relevant questions to our particular query â€” we don't have any exact matches but we can see that the returned questions are similar in the topics they are asking about. We can reformat this response to be a little easier to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gy7isg_f-vWg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79:  What's the world's largest city?\n",
      "0.73:  What is the biggest city?\n",
      "0.71:  What are the world's most advanced cities?\n",
      "0.7:  Where is the most beautiful city in the world?\n",
      "0.66:  What is the greatest, most beautiful city in the world?\n"
     ]
    }
   ],
   "source": [
    "for result in xc['matches']:\n",
    "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JK5yApl_5fE"
   },
   "source": [
    "These are good results, let's try and modify the words being used to see if we still surface similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJbjE-iq_yMr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64:  What is the biggest city?\n",
      "0.6:  What is the most dangerous city in USA?\n",
      "0.59:  What's the world's largest city?\n",
      "0.59:  What is the most dangerous city in USA? Why?\n",
      "0.58:  What are the world's most advanced cities?\n"
     ]
    }
   ],
   "source": [
    "query = \"which metropolis has the highest number of people?\"\n",
    "\n",
    "# create the query vector\n",
    "xq = model.encode(query).tolist()\n",
    "\n",
    "# now query\n",
    "xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
    "for result in xc['matches']:\n",
    "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIAxOPb-A2w_"
   },
   "source": [
    "Here we used different terms in our query than that of the returned documents. We substituted **\"city\"** for **\"metropolis\"** and **\"populated\"** for **\"number of people\"**.\n",
    "\n",
    "Despite these very different terms and *lack* of term overlap between query and returned documents â€” we get highly relevant results â€” this is the power of *semantic search*.\n",
    "\n",
    "You can go ahead and ask more questions above. When you're done, delete the index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cWdeKzhAtww"
   },
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B0zxR6hbf5d"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "llm-curso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
