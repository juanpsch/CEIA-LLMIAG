{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from htmlTemplates import css, bot_template, user_template\n",
    "\n",
    "from langchain.chains import RetrievalQA \n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "def get_vectorstore():\n",
    "    embd = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    \n",
    "    index_name = 'jptest'\n",
    "    namespace = \"espacio\"\n",
    "\n",
    "    vectorstore = PineconeVectorStore(index_name=index_name,embedding=embd,\n",
    "                                          namespace=namespace,)\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, Juan Pablo Schamun has experience in several areas, including industrial engineering, energy technology, and project management. He has a Master's degree in technological energy for sustainable development from the Polytechnic University of Valencia in Spain. He has also worked as a industrial engineer, business analyst, and contact engineer for ExxonMobil, where he analyzed business information and coordinated plant maintenance activities.\n",
      "\n",
      "In addition, Juan Pablo has experience as a independent consultant, providing consulting and advisory services in energy efficiency and renewable energy project design. He has also worked as a specialist for the CAREM project at the National Atomic Energy Commission (CNEA) in Argentina, where he was responsible for configuring 3D design tools, managing databases, creating 3D models, and coordinating a team of five people.\n",
      "\n",
      "Juan Pablo is also proficient in several computer programs and languages, including CATIA V5, ENOVIA V5, 3DVIAComposer, ENOVIA DMU, PDM administration, 3D CAD Modelling, 3DExperience Platform, Smart Plant Instrumentation, ISOGEN, Linux administration, Windows Server, virtualization, Oracle DBA, Autocad, and MS Project. He also has advanced knowledge of SQL and intermediate knowledge of Python and Visual Basic.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index_name = 'jptest'\n",
    "namespace = \"espacio\"\n",
    "embd = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# create vector store\n",
    "vectorstore = PineconeVectorStore(index_name=index_name,embedding=embd,\n",
    "                                        namespace=namespace,)\n",
    "\n",
    "query = \"Que experiencia tiene Juan Pablo\"\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(  \n",
    "    llm=chat,  \n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=vectorstore.as_retriever()  \n",
    ")  \n",
    "\n",
    "result = qa.invoke(query)\n",
    "print(result['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app_chat_doc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
